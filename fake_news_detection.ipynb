{"cells": [{"cell_type": "code", "metadata": {}, "source": ["\n", "# \ud83d\udcf0 Fake News Detection Using TF-IDF\n", "\n", "# \ud83d\udce6 Import Libraries\n", "import pandas as pd\n", "import numpy as np\n", "import re\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from wordcloud import WordCloud\n", "\n", "import nltk\n", "from nltk.corpus import stopwords\n", "from nltk.stem import WordNetLemmatizer\n", "\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.feature_extraction.text import TfidfVectorizer\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n", "\n", "# \ud83d\udce5 Load and Combine Dataset\n", "df_fake = pd.read_csv(\"Fake.csv\")\n", "df_real = pd.read_csv(\"True.csv\")\n", "df_fake['label'] = 0\n", "df_real['label'] = 1\n", "data = pd.concat([df_fake, df_real])\n", "data = data.sample(frac=1).reset_index(drop=True)\n", "data['text'] = data['title'] + \" \" + data['text']\n", "\n", "# \ud83e\uddf9 Data Cleaning & Preprocessing\n", "nltk.download('stopwords')\n", "nltk.download('wordnet')\n", "stop_words = set(stopwords.words('english'))\n", "lemmatizer = WordNetLemmatizer()\n", "\n", "def clean_text(text):\n", "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n", "    words = text.lower().split()\n", "    words = [lemmatizer.lemmatize(w) for w in words if w not in stop_words]\n", "    return ' '.join(words)\n", "\n", "data['clean_text'] = data['text'].apply(clean_text)\n", "\n", "# \u2601\ufe0f WordCloud Visualization\n", "fake_news = data[data['label'] == 0]['clean_text'].str.cat(sep=' ')\n", "real_news = data[data['label'] == 1]['clean_text'].str.cat(sep=' ')\n", "\n", "plt.figure(figsize=(12, 6))\n", "plt.subplot(1, 2, 1)\n", "plt.imshow(WordCloud(width=800, height=400).generate(fake_news))\n", "plt.title('Fake News WordCloud')\n", "\n", "plt.subplot(1, 2, 2)\n", "plt.imshow(WordCloud(width=800, height=400).generate(real_news))\n", "plt.title('Real News WordCloud')\n", "plt.show()\n", "\n", "# \ud83d\udd21 TF-IDF Vectorization\n", "tfidf = TfidfVectorizer(max_features=5000)\n", "X = tfidf.fit_transform(data['clean_text']).toarray()\n", "y = data['label']\n", "\n", "# \ud83d\udcca Train/Test Split\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "\n", "# \ud83e\udd16 Model Training (Logistic Regression)\n", "model = LogisticRegression()\n", "model.fit(X_train, y_train)\n", "y_pred = model.predict(X_test)\n", "\n", "# \ud83d\udcc8 Evaluation Metrics\n", "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n", "\n", "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n", "plt.title('Confusion Matrix')\n", "plt.xlabel('Predicted')\n", "plt.ylabel('True')\n", "plt.show()\n", "\n", "print(classification_report(y_test, y_pred))\n"], "outputs": [], "execution_count": null}], "metadata": {}, "nbformat": 4, "nbformat_minor": 2}